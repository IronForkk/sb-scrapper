# SB-Scrapper

SeleniumBase tabanlÄ±, anti-bot detection korumalÄ± web scraping API'si. Intranet kullanÄ±mÄ± iÃ§in optimize edilmiÅŸ, tamamen senkron Ã§alÄ±ÅŸan bir web scraping Ã§Ã¶zÃ¼mÃ¼dÃ¼r.

## ğŸ“‹ Ä°Ã§indekiler

- [Ã–zellikler](#Ã¶zellikler)
- [Teknik Mimari](#teknik-mimari)
- [Kurulum](#kurulum)
- [KullanÄ±m](#kullanÄ±m)
- [API DokÃ¼mantasyonu](#api-dokÃ¼mantasyonu)
- [KonfigÃ¼rasyon](#konfigÃ¼rasyon)
- [Proje YapÄ±sÄ±](#proje-yapÄ±sÄ±)
- [GeliÅŸtirme KurallarÄ±](#geliÅŸtirme-kurallarÄ±)
- [Lisans](#lisans)

## âœ¨ Ã–zellikler

### Web Scraping
- **Ã‡oklu Tarama Modu:** Ham URL ve ana domain taramasÄ±
- **Mobil GÃ¶rÃ¼nÃ¼m:** Mobil cihaz ekran gÃ¶rÃ¼ntÃ¼leri (375x812)
- **Arama Motoru Entegrasyonu:** Google ve DuckDuckGo sonuÃ§larÄ±
- **HTML Kaynak Kodu:** Sayfa kaynak kodlarÄ±nÄ± alma (Base64 formatÄ±nda)

### Anti-Bot Detection
- **Canvas Noise:** Canvas fingerprinting bypass
- **JS Sentinel:** Popup ve engelleyici element temizleme
- **User Agent RastgeleleÅŸtirme:** Windows, macOS, Linux platformlarÄ±
- **WebDriver Tespit Ã–nleme:** SeleniumBase ile geliÅŸmiÅŸ gizlilik
- **Black-list KorumasÄ±:** Ä°stenmeyen domain'leri filtreleme

### Loglama
- **PostgreSQL Loglama:** TÃ¼m loglar veritabanÄ±nda saklanÄ±r
- **Request Logging:** Ä°stek detaylarÄ± (headers, query params, body)
- **Error Logging:** Hatalar ayrÄ± tabloda saklanÄ±r
- **Domain Ä°statistikleri:** BaÅŸarÄ±/baÅŸarÄ±sÄ±z oranlarÄ± takibi

## ğŸ—ï¸ Teknik Mimari

### Temel Prensipler
- **Tamamen Senkron:** async/await, threading, multiprocessing YASAK
- **Tek Ä°stek Modu:** SÄ±ralÄ± istek iÅŸleme, paralel istek YASAK
- **Merkezi Loglama:** TÃ¼m loglar PostgreSQL'e
- **Intranet UygulamasÄ±:** Rate limiting, authentication, CORS YASAK

### Teknoloji YÄ±ÄŸÄ±nÄ±
- **Web Framework:** FastAPI
- **Browser Automation:** SeleniumBase
- **VeritabanÄ±:** PostgreSQL (psycopg2 - senkron)
- **Loglama:** Loguru + PostgreSQL
- **Validasyon:** Pydantic
- **KonfigÃ¼rasyon:** Pydantic Settings + python-dotenv

## ğŸš€ Kurulum

### Gereksinimler
- Python 3.11+
- Docker & Docker Compose (opsiyonel)
- PostgreSQL 15+

### Docker ile Kurulum (Ã–nerilen)

1. **Depoyu klonlayÄ±n:**
```bash
git clone <repository-url>
cd sb-scrapper
```

2. **.env dosyasÄ±nÄ± oluÅŸturun:**
```bash
cp .env.example .env
```

3. **.env dosyasÄ±nÄ± dÃ¼zenleyin:**
```bash
# PostgreSQL ÅŸifresini belirleyin
POSTGRES_PASSWORD=gÃ¼venli_sifre_buraya
```

4. **Docker Compose ile baÅŸlatÄ±n:**
```bash
docker-compose up -d
```

5. **Servislerin durumunu kontrol edin:**
```bash
docker-compose ps
```

### Manuel Kurulum

1. **Python sanal ortamÄ± oluÅŸturun:**
```bash
python3.11 -m venv venv
source venv/bin/activate  # Linux/macOS
# veya
venv\Scripts\activate  # Windows
```

2. **BaÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kleyin:**
```bash
pip install -r requirements.txt
```

3. **PostgreSQL veritabanÄ±nÄ± baÅŸlatÄ±n:**
```bash
# init.sql dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±n
psql -U postgres -d postgres -f db/init.sql
```

4. **.env dosyasÄ±nÄ± oluÅŸturun ve dÃ¼zenleyin:**
```bash
cp .env.example .env
```

5. **UygulamayÄ± baÅŸlatÄ±n:**
```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 1
```

## ğŸ“– KullanÄ±m

### API Endpoint

**POST /scrape**

Web sitesini tarar ve analiz eder.

### Ã–rnek Ä°stek

```bash
curl -X POST "http://localhost:8000/scrape" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://example.com",
    "wait_time": 8,
    "process_raw_url": true,
    "process_main_domain": true,
    "get_html": true,
    "get_mobile_ss": true,
    "get_google_search": true,
    "get_google_html": true,
    "get_ddg_search": true,
    "get_ddg_html": true
  }'
```

### Ã–rnek YanÄ±t

```json
{
  "success": true,
  "data": {
    "url": "https://example.com",
    "domain": "example.com",
    "screenshot_raw_url": "base64_encoded_image",
    "screenshot_mobile": "base64_encoded_image",
    "html_raw_url": "base64_encoded_html",
    "google_search_ss": "base64_encoded_image",
    "google_html": "base64_encoded_html",
    "ddg_search_ss": "base64_encoded_image",
    "ddg_html": "base64_encoded_html",
    "screenshot_main_domain": "base64_encoded_image",
    "html_main_domain": "base64_encoded_html",
    "logs": [
      {
        "timestamp": "2024-01-01T12:00:00Z",
        "level": "INFO",
        "message": "Ä°ÅŸlem baÅŸladÄ±"
      }
    ]
  },
  "execution_time": 15.5
}
```

### Python Ä°stemci Ã–rneÄŸi

```python
import requests

url = "http://localhost:8000/scrape"
payload = {
    "url": "https://example.com",
    "wait_time": 8,
    "process_raw_url": True,
    "process_main_domain": True,
    "get_html": True,
    "get_mobile_ss": True,
    "get_google_search": True,
    "get_google_html": True,
    "get_ddg_search": True,
    "get_ddg_html": True
}

response = requests.post(url, json=payload)
result = response.json()

if result["success"]:
    print(f"Ä°ÅŸlem baÅŸarÄ±lÄ±! SÃ¼re: {result['execution_time']} saniye")
else:
    print(f"Hata: {result['error']}")
```

## ğŸ“š API DokÃ¼mantasyonu

### Swagger UI
Uygulama baÅŸlatÄ±ldÄ±ÄŸÄ±nda otomatik olarak oluÅŸturulur:
- **Swagger UI:** http://localhost:8000/docs
- **ReDoc:** http://localhost:8000/redoc
- **OpenAPI JSON:** http://localhost:8000/openapi.json

### Health Check
```bash
curl http://localhost:8000/health
```

YanÄ±t:
```json
{
  "status": "healthy",
  "postgres_connected": true
}
```

## âš™ï¸ KonfigÃ¼rasyon

### .env DosyasÄ±

TÃ¼m ayarlar `.env` dosyasÄ±ndan yÃ¶netilir. Ã–rnek ayarlar:

#### TarayÄ±cÄ± AyarlarÄ±
```env
HEADLESS=true                    # Headless mod
WAIT_TIME=8                      # Sayfa yÃ¼kleme sonrasÄ± bekleme
USER_AGENT_PLATFORM=windows      # User Agent platform
PAGE_LOAD_TIMEOUT=60            # Sayfa yÃ¼kleme zaman aÅŸÄ±mÄ±
BODY_CHECK_WAIT_TIME=2          # JS yÃ¼klenme bekleme sÃ¼resi
PAGE_RELOAD_WAIT_TIME=5         # Sayfa yenileme bekleme sÃ¼resi
```

#### API AyarlarÄ±
```env
HOST=0.0.0.0                    # Dinlenecek IP
PORT=8000                       # Dinlenecek port
```

#### Loglama AyarlarÄ±
```env
LOG_LEVEL=INFO                  # Log seviyesi
CONSOLE_LOGGING_ENABLED=true    # Konsol loglama
POSTGRES_LOGGING_ENABLED=true   # PostgreSQL loglama
STRUCTURED_LOGGING_ENABLED=false # JSON format loglama
```

#### Canvas Noise AyarlarÄ±
```env
NOISE_MIN_VALUE=-20             # Minimum noise deÄŸeri
NOISE_MAX_VALUE=20              # Maksimum noise deÄŸeri
```

#### Black-List AyarlarÄ±
```env
BLACKLIST_FILE=black-list.lst   # Black-list dosya yolu
```

#### PostgreSQL AyarlarÄ±
```env
POSTGRES_HOST=postgres          # PostgreSQL host
POSTGRES_PORT=5432              # PostgreSQL port
POSTGRES_DB=sb_scrapper         # VeritabanÄ± adÄ±
POSTGRES_USER=sb_user           # KullanÄ±cÄ± adÄ±
POSTGRES_PASSWORD=gÃ¼venli_sifre # Åifre
```

## ğŸ“ Proje YapÄ±sÄ±

```
sb-scrapper/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI ana uygulama (/scrape endpoint)
â”‚   â”œâ”€â”€ config.py            # .env ayarlarÄ± yÃ¶netimi
â”‚   â”œâ”€â”€ schemas.py           # Pydantic request/response modelleri
â”‚   â”œâ”€â”€ swagger_config.py    # Swagger dokÃ¼mantasyonu
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ browser.py       # SeleniumBase wrapper (senkron)
â”‚   â”‚   â”œâ”€â”€ logger.py        # Loguru logger
â”‚   â”‚   â”œâ”€â”€ postgres_logger.py  # PostgreSQL logger (senkron)
â”‚   â”‚   â””â”€â”€ blacklist.py     # Black-list yÃ¶netimi
â”‚   â”œâ”€â”€ payloads/
â”‚   â”‚   â”œâ”€â”€ noise_js.py      # Canvas noise JS (DOKUNMA!)
â”‚   â”‚   â””â”€â”€ sentinel_js.py   # Sentinel JS (DOKUNMA!)
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â””â”€â”€ connection.py    # PostgreSQL baÄŸlantÄ±sÄ± (senkron)
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ user_agents.py   # User Agent listesi
â”œâ”€â”€ db/
â”‚   â””â”€â”€ init.sql             # VeritabanÄ± ÅŸemasÄ±
â”œâ”€â”€ static/
â”‚   â””â”€â”€ swagger-ui.css       # Swagger UI CSS
â”œâ”€â”€ black-list.lst           # Black-list domain listesi
â”œâ”€â”€ .env                     # Ayarlar (oluÅŸturulmalÄ±)
â”œâ”€â”€ .env.example             # Ã–rnek ayarlar
â”œâ”€â”€ docker-compose.yml       # Docker Compose konfigÃ¼rasyonu
â”œâ”€â”€ Dockerfile               # Docker imajÄ±
â”œâ”€â”€ requirements.txt         # Python baÄŸÄ±mlÄ±lÄ±klarÄ±
â””â”€â”€ README.md                # Bu dosya
```

## ğŸ“œ GeliÅŸtirme KurallarÄ±

### Ä°zin Verilen Paketler
| Paket | KullanÄ±m AlanÄ± |
|-------|---------------|
| `fastapi` | Web framework (senkron mod) |
| `seleniumbase` | Browser automation |
| `loguru` | Logging |
| `psycopg2` | PostgreSQL (senkron) |
| `pydantic` | Validation |
| `pydantic-settings` | Config |
| `requests` | HTTP (senkron) |
| `httpx` | HTTP (senkron mod) |

## ğŸ”’ GÃ¼venlik

- **Black-list:** Ä°stenmeyen domain'ler filtrelenir
- **Canvas Noise:** Fingerprinting tespiti zorlaÅŸtÄ±rÄ±lÄ±r
- **WebDriver GizliliÄŸi:** SeleniumBase ile geliÅŸmiÅŸ gizlilik
- **User Agent RastgeleleÅŸtirme:** Her oturum iÃ§in farklÄ± UA
- **Response Body Loglama YOK:** Hassas veriler loglanmaz

## ğŸ“Š VeritabanÄ±

### Tablolar

1. **application_logs:** Uygulama loglarÄ±
2. **request_logs:** Ä°stek loglarÄ± (response body hariÃ§)
3. **error_logs:** Hata loglarÄ±
4. **domain_stats:** Domain istatistikleri

### Log SorgularÄ±

```sql
-- Son 10 log
SELECT * FROM application_logs ORDER BY timestamp DESC LIMIT 10;

-- HatalÄ± istekler
SELECT * FROM error_logs ORDER BY timestamp DESC;

-- Domain istatistikleri
SELECT * FROM domain_stats ORDER BY timestamp DESC;
```

## ğŸ› Hata AyÄ±klama

### LoglarÄ± GÃ¶rÃ¼ntÃ¼leme

```bash
# Docker loglarÄ±
docker-compose logs -f sb-scraper

# PostgreSQL loglarÄ±
docker-compose logs -f postgres
```

### VeritabanÄ±na BaÄŸlanma

```bash
docker exec -it sb-postgres psql -U sb_user -d sb_scrapper
```

### Test Ä°steÄŸi

```bash
curl -X POST "http://localhost:8000/scrape" \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com"}'
```

## ğŸ“„ Lisans

Bu proje [LICENSE](LICENSE) dosyasÄ±nda belirtilen lisans altÄ±nda lisanslanmÄ±ÅŸtÄ±r.

**SB-Scrapper v3.0.0** - SeleniumBase tabanlÄ± web scraping API'si
